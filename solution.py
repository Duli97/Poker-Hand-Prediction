# -*- coding: utf-8 -*-
"""Solution_184181J.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SrKFgcT071uqHHioUhC7tfAnT72JnEcV

# **Assignment - Individual 184181J**
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
import datetime, os

# Commented out IPython magic to ensure Python compatibility.
#Load the Tensorboard notebook extension
# %load_ext tensorboard

"""## **Preprocessing**"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""Importing the dataset """

dataset_training = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Assignment/poker-hand-training-true.data', header=None)
dataset_testing = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Assignment/poker-hand-testing.data', header=None)

#Assign column names for each column
dataset_training.columns =['S1', 'C1', 'S2', 'C2', 'S3' , 'C3', 'S4', 'C4', 'S5', 'C5', 'CLASS']
dataset_testing.columns =['S1', 'C1', 'S2', 'C2', 'S3' , 'C3', 'S4', 'C4', 'S5', 'C5', 'CLASS']

dataset_training

dataset_testing

"""One Hot Encoding (Columns - 'S1', 'C1', 'S2', 'C2', 'S3' , 'C3', 'S4', 'C4', 'S5', 'C5')

"""

#FOR TRAINING SET


#These columns will encoded in one-hot encoding.
categorical_cols = ['S1', 'C1', 'S2', 'C2', 'S3' , 'C3', 'S4', 'C4', 'S5', 'C5'] 

#Convert integer value to string value before apply one hot encoding
dataset_training[categorical_cols] = dataset_training[categorical_cols].astype(str)

#One-hot-encode the categorical columns.
data_training_hot_encoded = pd.get_dummies(dataset_training[categorical_cols])

#Extract only the columns that didnt need to be encoded
data_training_other_cols = dataset_training.drop(columns=categorical_cols)

#Concatenate the two dataframes : 
data_training_out = pd.concat([data_training_hot_encoded, data_training_other_cols], axis=1)

#Training dataset after one hot encoding
data_training_out

#Matrix of features
X_train = data_training_out.iloc[0:, :-1].values

#Dependent variable vector
y_train = data_training_out.iloc[:, -1].values

#FOR TESTING SET


#These columns will encoded in one-hot encoding.
categorical_cols = ['S1', 'C1', 'S2', 'C2', 'S3' , 'C3', 'S4', 'C4', 'S5', 'C5'] 

#Convert integer value to string value before apply one hot encoding
dataset_testing[categorical_cols] = dataset_testing[categorical_cols].astype(str)

#One-hot-encode the categorical columns.
data_testing_hot_encoded = pd.get_dummies(dataset_testing[categorical_cols])

#Extract only the columns that didnt need to be encoded
data_testing_other_cols = dataset_testing.drop(columns=categorical_cols)

#Concatenate the two dataframes : 
data_testing_out = pd.concat([data_testing_hot_encoded, data_testing_other_cols], axis=1)

#Testing dataset after one hot encoding
data_testing_out

#Matrix of features
X_test = data_testing_out.iloc[0:, :-1].values

#Dependent variable vector
y_test = data_testing_out.iloc[:, -1].values

X_test

y_test

"""## **Building & Training the ANN**"""

import tensorflow as tf

#without momentum learning and without early stopping
logdir_1 = "logs/image_1/"
tensorboard_callback_1 = tf.keras.callbacks.TensorBoard(log_dir = logdir_1, histogram_freq = 1)
file_writer_cm = tf.summary.create_file_writer(logdir_1 + '/cm')

#with momentum learning but without early stopping
logdir_2 = "logs/image_2/"
tensorboard_callback_2 = tf.keras.callbacks.TensorBoard(log_dir = logdir_2, histogram_freq = 1)
file_writer_cm = tf.summary.create_file_writer(logdir_2 + '/cm')

#with momentum learning and with early stopping
logdir_3 = "logs/image_3/"
tensorboard_callback_3 = tf.keras.callbacks.TensorBoard(log_dir = logdir_3, histogram_freq = 1)
file_writer_cm = tf.summary.create_file_writer(logdir_3 + '/cm')

#Without momentum learning and without early stopping


#Building the model
model_1 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(30, activation='relu'),
    tf.keras.layers.Dense(120, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax'),
])

#Compiling the model
model_1.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

#Training the model on training set with getting a validation set 
model_1.fit(X_train, y_train, batch_size = 32, validation_split = 0.2, epochs = 100, callbacks=[tensorboard_callback_1])

#With momentum learning (but without early stopping)


model_2 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(units=30, activation='relu'),
    tf.keras.layers.Dense(units=120, activation='relu'),
    tf.keras.layers.Dense(units=10, activation='softmax'),
])

#Defining the SGD with momentum 
opt = tf.keras.optimizers.SGD(
    learning_rate=0.01, momentum=0.95, nesterov=False, name="SGD"
)

#Compiling the model
model_2.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

#Training the model on training set with getting a validation set 
model_2.fit(X_train, y_train, batch_size = 32, validation_split = 0.2, epochs = 100, callbacks=[tensorboard_callback_2])

#Defining the early stopping object 'es'
es = tf.keras.callbacks.EarlyStopping(
    monitor = 'val_loss',
    min_delta=0,
    patience=15,
    verbose=1,
    mode='min',
    baseline=None,
    restore_best_weights=False
)

#With momentum learning and with early stopping


model_3 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(units=30, activation='relu'),
    tf.keras.layers.Dense(units=120, activation='relu'),
    tf.keras.layers.Dense(units=10, activation='softmax'),
])

#Defining the SGD with momentum 
opt = tf.keras.optimizers.SGD(
    learning_rate=0.01, momentum=0.95, nesterov=False, name="SGD"
)

#Compiling the model
model_3.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

#Training the model on training set with getting a validation set 
model_3.fit(X_train, y_train, batch_size = 32, validation_split = 0.2, epochs = 100, callbacks=[tensorboard_callback_3, es])

"""## **Predict the value of Y for the test dataset**

With early Stopping
"""

#Predicting the test results

accuracy_with_es = model_3.evaluate(X_test, y_test)
print("Test loss and test accuracy with early stopping", accuracy_with_es)

"""Without early Stopping"""

#Predicting the test results

accuracy_without_es = model_2.evaluate(X_test, y_test)
print("Test loss and test accuracy with early stopping", accuracy_without_es)

"""# **Hyperparameter tuning**"""

!pip install keras-tuner -q

#Import  hparamâ€™ api module
from tensorboard.plugins.hparams import api as hp

#Create a new log directory
logdir = "logs/hparamas"

HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([20, 30]))
HP_LOSS = hp.HParam('loss', hp.Discrete(['sparse_categorical_crossentropy', 'kl_divergence']))
HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu', 'sigmoid', 'tanh']))

# tf.summary.create_file_writer() method to write to logs folder

METRIC_ACCURACY = 'accuracy'

with tf.summary.create_file_writer(logdir).as_default():
    hp.hparams_config(
        hparams=[HP_NUM_UNITS, HP_LOSS, HP_ACTIVATION],
        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],)

def create_model(hparams):
    #Defining the SGD with momentum 
    opti = tf.keras.optimizers.SGD(
        learning_rate=0.01, momentum=0.95, nesterov=False, name="SGD"
    )

    model = tf.keras.models.Sequential([
        tf.keras.layers.Dense(hparams[HP_NUM_UNITS],  activation=hparams[HP_ACTIVATION]),
        tf.keras.layers.Dense(120, activation=hparams[HP_ACTIVATION]),
        tf.keras.layers.Dense(units=10, activation='softmax')])

    model.compile(optimizer=opti,
                  loss=hparams[HP_LOSS],
                  metrics=['accuracy'])

    model.fit(X_train, y_train, batch_size = 32, validation_split = 0.2, epochs=100)
    loss, accuracy = model.evaluate(X_test, y_test)

    return accuracy

#A run function that would log a summary of `hparams` with final accuracy and hyperparameters

def experiment(experiment_dir, hparams):

    with tf.summary.create_file_writer(experiment_dir).as_default():
        hp.hparams(hparams)
        accuracy = create_model(hparams)
        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)

#Training our model using different sets of hyperparameters, for this example, we are going to try a number of combinations including upper and lower bound of real-valued parameters

experiment_no = 0

for num_units in HP_NUM_UNITS.domain.values:
    for activation_func in HP_ACTIVATION.domain.values:
        for loss_func in HP_LOSS.domain.values:
            hparams = {
                HP_NUM_UNITS: num_units,
                HP_ACTIVATION: activation_func,
                HP_LOSS: loss_func,}

            experiment_name = f'Experiment {experiment_no}'
            print(f'Starting Experiment: {experiment_name}')
            print({h.name: hparams[h] for h in hparams})
            experiment(logdir + experiment_name, hparams)
            experiment_no += 1

"""# **Tensorboard**"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs

"""**Answer for the part (b)**

> From the above **epoch_accuracy** graph we can see that the model_2 validation accuracy(image_2/validation) is higher than model_1 validation accuracy(image_1/validation). And also it can be seen that model_2 gained an accuracy of more than 0.95 with smaller number of epochs than model 2.

**Answer for the part (c)**

> From the above **epoch_loss** graph we can see that the model_3 (model with early stopping) has stopped training after severel steps. Because from that step the loss value started to increase therefore the model started to overfit to the dataset.

**Answer for the part (d)**

> By looking at "accuracy" graph on the "SCALARS" tab we can see that hparamasExperiment 7 has the highest accuracy. 
The tuned hyperparameters are as follows:


*   Number of neurons in the first hidden layer = 30
*   Activation function for hidden layers = relu
*   Loss function = sparse_categorical_crossentropy
"""