# -*- coding: utf-8 -*-
"""Solution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SrKFgcT071uqHHioUhC7tfAnT72JnEcV

# **Assignment - Individual 184181J**
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
import datetime, os

# Commented out IPython magic to ensure Python compatibility.
#Load the Tensorboard notebook extension
# %load_ext tensorboard

"""## **Preprocessing**"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""Importing the dataset """

dataset_training = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Assignment/poker-hand-training-true.data', header=None)
dataset_testing = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Assignment/poker-hand-testing.data', header=None)

#Assign column names for each column
dataset_training.columns =['S1', 'C1', 'S2', 'C2', 'S3' , 'C3', 'S4', 'C4', 'S5', 'C5', 'CLASS']
dataset_testing.columns =['S1', 'C1', 'S2', 'C2', 'S3' , 'C3', 'S4', 'C4', 'S5', 'C5', 'CLASS']

dataset_training

dataset_testing

"""One Hot Encoding (Columns - 'S1', 'C1', 'S2', 'C2', 'S3' , 'C3', 'S4', 'C4', 'S5', 'C5')

"""

#FOR TRAINING SET


#These columns will encoded in one-hot encoding.
categorical_cols = ['S1', 'C1', 'S2', 'C2', 'S3' , 'C3', 'S4', 'C4', 'S5', 'C5'] 

#Convert integer value to string value before apply one hot encoding
dataset_training[categorical_cols] = dataset_training[categorical_cols].astype(str)

#One-hot-encode the categorical columns.
data_training_hot_encoded = pd.get_dummies(dataset_training[categorical_cols])

#Extract only the columns that didnt need to be encoded
data_training_other_cols = dataset_training.drop(columns=categorical_cols)

#Concatenate the two dataframes : 
data_training_out = pd.concat([data_training_hot_encoded, data_training_other_cols], axis=1)

#Training dataset after one hot encoding
data_training_out

#Matrix of features
X_train = data_training_out.iloc[0:, :-1].values

#Dependent variable vector
y_train = data_training_out.iloc[:, -1].values

#FOR TRAINING SET


#These columns will encoded in one-hot encoding.
categorical_cols = ['S1', 'C1', 'S2', 'C2', 'S3' , 'C3', 'S4', 'C4', 'S5', 'C5'] 

#Convert integer value to string value before apply one hot encoding
dataset_testing[categorical_cols] = dataset_testing[categorical_cols].astype(str)

#One-hot-encode the categorical columns.
data_testing_hot_encoded = pd.get_dummies(dataset_testing[categorical_cols])

#Extract only the columns that didnt need to be encoded
data_testing_other_cols = dataset_testing.drop(columns=categorical_cols)

#Concatenate the two dataframes : 
data_testing_out = pd.concat([data_testing_hot_encoded, data_testing_other_cols], axis=1)

#Training dataset after one hot encoding
data_testing_out

#Matrix of features
X_test = data_testing_out.iloc[0:, :-1].values

#Dependent variable vector
y_test = data_testing_out.iloc[:, -1].values

X_test

y_test

"""## **Building & Training the ANN**"""

import tensorflow as tf

#without momentum learning and without early stopping
logdir_1 = "logs/image_1/"
tensorboard_callback_1 = tf.keras.callbacks.TensorBoard(log_dir = logdir_1, histogram_freq = 1)
file_writer_cm = tf.summary.create_file_writer(logdir_1 + '/cm')

#with momentum learning but without early stopping
logdir_2 = "logs/image_2/"
tensorboard_callback_2 = tf.keras.callbacks.TensorBoard(log_dir = logdir_2, histogram_freq = 1)
file_writer_cm = tf.summary.create_file_writer(logdir_2 + '/cm')

#with momentum learning and with early stopping
logdir_3 = "logs/image_3/"
tensorboard_callback_3 = tf.keras.callbacks.TensorBoard(log_dir = logdir_3, histogram_freq = 1)
file_writer_cm = tf.summary.create_file_writer(logdir_3 + '/cm')

#Without momentum learning and without early stopping


#Building the model
model_1 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(48, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax'),
])

#Compiling the model
model_1.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

#Training the model on training set with getting a validation set 
model_1.fit(X_train, y_train, batch_size = 32, validation_split = 0.2, epochs = 200, callbacks=[tensorboard_callback_1])

#With momentum learning (but without early stopping)


model_2 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(units=64, activation='relu'),
    tf.keras.layers.Dense(units=48, activation='relu'),
    tf.keras.layers.Dense(units=10, activation='softmax'),
])

#Defining the SGD with momentum 
opt = tf.keras.optimizers.SGD(
    learning_rate=0.01, momentum=0.95, nesterov=False, name="SGD"
)

#Compiling the model
model_2.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

#Training the model on training set with getting a validation set 
model_2.fit(X_train, y_train, batch_size = 32, validation_split = 0.2, epochs = 200, callbacks=[tensorboard_callback_2])

#Defining the early stopping object 'es'
es = tf.keras.callbacks.EarlyStopping(
    monitor = 'val_loss',
    min_delta=0,
    patience=15,
    verbose=1,
    mode='min',
    baseline=None,
    restore_best_weights=False
)

#With momentum learning and with early stopping)


model_3 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(units=64, activation='relu'),
    tf.keras.layers.Dense(units=48, activation='relu'),
    tf.keras.layers.Dense(units=10, activation='softmax'),
])

#Defining the SGD with momentum 
opt = tf.keras.optimizers.SGD(
    learning_rate=0.01, momentum=0.95, nesterov=False, name="SGD"
)

#Compiling the model
model_3.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

#Training the model on training set with getting a validation set 
model_3.fit(X_train, y_train, batch_size = 32, validation_split = 0.2, epochs = 200, callbacks=[tensorboard_callback_3, es])

"""## **Predict the value of Y for the test dataset**

With early Stopping
"""

#Predicting the test results

accuracy_with_es = model_3.evaluate(X_test, y_test)
print("Test loss and test accuracy with early stopping", accuracy_with_es)

"""Without early Stopping"""

#Predicting the test results

accuracy_without_es = model_2.evaluate(X_test, y_test)
print("Test loss and test accuracy with early stopping", accuracy_without_es)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs

"""**Answer for the part (b)**

> From the above **epoch_accuracy** graph we can see that the model_2 validation accuracy(image_2/validation) is higher than model_1 validation accuracy(image_1/validation). And also it can be seen that model_2 gained an accuracy of more than 0.95 with smaller number of epochs than model 2.

**Answer for the part (c)**

> From the above **epoch_loss** graph we can see that the model_3 (model with early stopping) has stopped training after severel steps. Because from that step the loss value started to increase therefore the model started to overfit to the dataset.

**Answer for the part (d)**

> By looking at histograms on tensorboard we can see that weights of the neurons in each layer has normal distibutions. So it means that the number of neurons in 1st hidden layer is optimized and the activation function, loss function are also optimized.
"""